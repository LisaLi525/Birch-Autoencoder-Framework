{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIRCH-AE: BIRCH Autoencoder Ensemble Framework\n",
    "\n",
    "**A scalable hierarchical ensemble clustering framework for e-commerce user segmentation**\n",
    "\n",
    "This framework integrates:\n",
    "- Deep autoencoder-based dimensionality reduction for handling high-dimensional correlated features\n",
    "- BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) with multiple configurations\n",
    "- Hierarchical ensemble consensus methods (Majority Voting, Weighted Voting, AASC, BOHC/CSPA)\n",
    "- Dynamic ensemble selection with multi-criteria evaluation\n",
    "- Incremental learning support for streaming data\n",
    "\n",
    "## Key Innovations:\n",
    "1. **Hierarchical Ensemble Architecture**: Multiple BIRCH configurations with varying threshold values\n",
    "2. **Autoencoder Feature Learning**: Non-linear dimensionality reduction optimized for BIRCH clustering\n",
    "3. **BIRCH-Optimized Consensus**: Hierarchical consensus methods (BOHC/CSPA) designed for BIRCH\n",
    "4. **Dynamic Selection**: Automatic strategy selection using Silhouette, Calinski-Harabasz, Davies-Bouldin\n",
    "5. **Memory Efficiency**: Leverages BIRCH's CF Tree structure for large-scale datasets\n",
    "6. **Incremental Learning**: Supports streaming data with real-time segment updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install tensorflow==2.18.0\n",
    "# !pip install numpy==2.0.2\n",
    "# !pip install scikit-learn\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import Birch, SpectralClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from scipy.stats import mode\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, LeakyReLU, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(filepath, user_id_col='user_id', sample_size=None, test_size=0.3):\n",
    "    \"\"\"\n",
    "    Load and preprocess data with automatic handling of numeric and categorical features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to the CSV file\n",
    "    user_id_col : str\n",
    "        Name of the user ID column (will be used as index)\n",
    "    sample_size : int, optional\n",
    "        Number of samples to use (for large datasets)\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing (default: 0.3)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    train_processed : ndarray\n",
    "        Preprocessed training data\n",
    "    test_processed : ndarray\n",
    "        Preprocessed test data\n",
    "    train_ids : ndarray\n",
    "        Training user IDs\n",
    "    test_ids : ndarray\n",
    "        Test user IDs\n",
    "    preprocessor : ColumnTransformer\n",
    "        Fitted preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = pd.read_csv(filepath)\n",
    "    \n",
    "    # Use index as user_id if column doesn't exist\n",
    "    if user_id_col not in data.columns:\n",
    "        print(f\"Warning: '{user_id_col}' not found. Using dataset index as user_id.\")\n",
    "        data[user_id_col] = data.index\n",
    "    \n",
    "    # Set user_id as index\n",
    "    data.set_index(user_id_col, inplace=True)\n",
    "    \n",
    "    # Downsample if needed\n",
    "    if sample_size and len(data) > sample_size:\n",
    "        data = data.sample(n=sample_size, random_state=42)\n",
    "        print(f\"Downsampled to {sample_size} samples\")\n",
    "    \n",
    "    # Replace infinite values with NaN\n",
    "    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Identify valid features\n",
    "    numeric_features = [col for col in data.select_dtypes(include=[np.number]).columns \n",
    "                       if data[col].notna().sum() > 0]\n",
    "    categorical_features = [col for col in data.select_dtypes(include=[object, 'category']).columns \n",
    "                           if data[col].notna().sum() > 0]\n",
    "    \n",
    "    if not numeric_features and not categorical_features:\n",
    "        raise ValueError(\"No valid features found in dataset!\")\n",
    "    \n",
    "    print(f\"Found {len(numeric_features)} numeric and {len(categorical_features)} categorical features\")\n",
    "    \n",
    "    # Convert categorical features to string\n",
    "    for col in categorical_features:\n",
    "        data[col] = data[col].astype(str)\n",
    "    \n",
    "    # Define transformers\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', KNNImputer(n_neighbors=5)),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    # Create column transformer\n",
    "    transformers = []\n",
    "    if numeric_features:\n",
    "        transformers.append(('num', numeric_transformer, numeric_features))\n",
    "    if categorical_features:\n",
    "        transformers.append(('cat', categorical_transformer, categorical_features))\n",
    "    \n",
    "    preprocessor = ColumnTransformer(transformers=transformers)\n",
    "    \n",
    "    # Split data\n",
    "    train_df, test_df = train_test_split(data, test_size=test_size, random_state=42)\n",
    "    train_ids = train_df.index.values\n",
    "    test_ids = test_df.index.values\n",
    "    \n",
    "    # Fit and transform\n",
    "    preprocessor.fit(train_df)\n",
    "    train_processed = preprocessor.transform(train_df)\n",
    "    test_processed = preprocessor.transform(test_df)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    train_processed = np.array(train_processed)\n",
    "    test_processed = np.array(test_processed)\n",
    "    \n",
    "    # Clean data\n",
    "    train_processed = np.nan_to_num(train_processed, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    test_processed = np.nan_to_num(test_processed, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    print(f\"Preprocessing complete: Train shape {train_processed.shape}, Test shape {test_processed.shape}\")\n",
    "    \n",
    "    return train_processed, test_processed, train_ids, test_ids, preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autoencoder Feature Learning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(input_dim, latent_dim=14, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Build a deep autoencoder for dimensionality reduction.\n",
    "    Architecture: 512 -> 256 -> 128 -> latent_dim -> 128 -> 256 -> input_dim\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Input feature dimension\n",
    "    latent_dim : int\n",
    "        Latent space dimension (default: 14)\n",
    "    dropout_rate : float\n",
    "        Dropout rate for regularization (default: 0.3)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Model\n",
    "        Compiled autoencoder model\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    x = Dense(512)(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Latent space\n",
    "    encoded = Dense(latent_dim, activation='relu', name='encoded_layer')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Dense(128)(encoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    output_layer = Dense(input_dim, activation='linear')(x)\n",
    "    \n",
    "    # Compile model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_autoencoder(model, data, epochs=100, batch_size=64, patience=15):\n",
    "    \"\"\"\n",
    "    Train the autoencoder with early stopping.\n",
    "    Uses 10% of data for validation monitoring.\n",
    "    \"\"\"\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=patience, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        data, data,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.1,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def encode_data(autoencoder, data):\n",
    "    \"\"\"\n",
    "    Extract encoded representations from trained autoencoder.\n",
    "    Returns compact latent space representations optimized for BIRCH.\n",
    "    \"\"\"\n",
    "    encoder = Model(\n",
    "        inputs=autoencoder.input, \n",
    "        outputs=autoencoder.get_layer('encoded_layer').output\n",
    "    )\n",
    "    return encoder.predict(data, verbose=0)\n",
    "\n",
    "\n",
    "def reduce_dimensions_pca(data, n_components=14):\n",
    "    \"\"\"\n",
    "    Alternative: Reduce dimensions using PCA (faster, linear method).\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_data = pca.fit_transform(data)\n",
    "    print(f\"PCA explained variance ratio: {sum(pca.explained_variance_ratio_):.3f}\")\n",
    "    return reduced_data, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BIRCH Ensemble Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_birch_clustering(data, threshold=0.5, branching_factor=50, n_clusters=5):\n",
    "    \"\"\"\n",
    "    Apply BIRCH clustering with specific parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : ndarray\n",
    "        Input data (typically encoded representations)\n",
    "    threshold : float\n",
    "        Maximum diameter for subclusters at leaf nodes (controls granularity)\n",
    "    branching_factor : int\n",
    "        Maximum number of CF subclusters in each node (controls tree breadth)\n",
    "    n_clusters : int\n",
    "        Number of clusters for global clustering phase\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Clustering results with labels and metrics\n",
    "    \"\"\"\n",
    "    # Initialize BIRCH with parameters\n",
    "    birch = Birch(threshold=threshold, branching_factor=branching_factor, n_clusters=n_clusters)\n",
    "    \n",
    "    # Fit and predict\n",
    "    labels = birch.fit_predict(data)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'labels': labels,\n",
    "        'silhouette': silhouette_score(data, labels),\n",
    "        'calinski_harabasz': calinski_harabasz_score(data, labels),\n",
    "        'davies_bouldin': davies_bouldin_score(data, labels),\n",
    "        'threshold': threshold,\n",
    "        'branching_factor': branching_factor\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_birch_ensemble(data, n_clusters_range=[5, 10, 15, 20]):\n",
    "    \"\"\"\n",
    "    Run BIRCH ensemble with multiple parameter configurations.\n",
    "    \n",
    "    Creates ensemble by varying:\n",
    "    - Threshold values (fine-grained 0.3, balanced 0.5, coarse-grained 0.8)\n",
    "    - Number of clusters\n",
    "    \n",
    "    This captures diverse clustering perspectives at multiple granularities.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results_df : DataFrame\n",
    "        Results for all BIRCH configurations\n",
    "    labels_dict : dict\n",
    "        Dictionary of labels for each configuration\n",
    "    \"\"\"\n",
    "    # Define BIRCH parameter configurations\n",
    "    birch_configs = [\n",
    "        {'name': 'Fine-Grained', 'threshold': 0.3, 'branching_factor': 50},\n",
    "        {'name': 'Balanced', 'threshold': 0.5, 'branching_factor': 50},\n",
    "        {'name': 'Coarse-Grained', 'threshold': 0.8, 'branching_factor': 50},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    labels_dict = {}\n",
    "    \n",
    "    for n_clusters in n_clusters_range:\n",
    "        for config in birch_configs:\n",
    "            try:\n",
    "                result = apply_birch_clustering(\n",
    "                    data, \n",
    "                    threshold=config['threshold'],\n",
    "                    branching_factor=config['branching_factor'],\n",
    "                    n_clusters=n_clusters\n",
    "                )\n",
    "                \n",
    "                # Store results\n",
    "                model_key = f\"BIRCH_{config['name']}_{n_clusters}\"\n",
    "                labels_dict[model_key] = result['labels']\n",
    "                \n",
    "                results.append({\n",
    "                    'Model': f\"BIRCH-{config['name']}\",\n",
    "                    'Clusters': n_clusters,\n",
    "                    'Threshold': config['threshold'],\n",
    "                    'Silhouette': result['silhouette'],\n",
    "                    'Calinski-Harabasz': result['calinski_harabasz'],\n",
    "                    'Davies-Bouldin': result['davies_bouldin']\n",
    "                })\n",
    "                \n",
    "                print(f\"✓ {model_key}: Silhouette={result['silhouette']:.3f}, T={config['threshold']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Failed {config['name']} BIRCH with {n_clusters} clusters: {e}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df, labels_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensemble Consensus Strategies Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Method 1: Majority Voting (MV)\n",
    "def majority_voting_ensemble(labels_list):\n",
    "    \"\"\"\n",
    "    Simple majority voting across multiple BIRCH clustering results.\n",
    "    Assigns each user to the cluster label that appears most frequently.\n",
    "    \"\"\"\n",
    "    labels_array = np.column_stack(labels_list)\n",
    "    ensemble_labels, _ = mode(labels_array, axis=1, keepdims=False)\n",
    "    return ensemble_labels.flatten()\n",
    "\n",
    "\n",
    "# Ensemble Method 2: Weighted Voting (WV)\n",
    "def weighted_voting_ensemble(labels_list, weights):\n",
    "    \"\"\"\n",
    "    Weighted voting based on clustering quality (silhouette scores).\n",
    "    Higher quality clusterings have more influence on the final consensus.\n",
    "    Weight formula: w_m = exp(β * S_m) / Σ exp(β * S_j)\n",
    "    \"\"\"\n",
    "    n_samples = len(labels_list[0])\n",
    "    n_clusters = max([len(np.unique(labels)) for labels in labels_list])\n",
    "    weighted_labels = np.zeros((n_samples, n_clusters))\n",
    "    \n",
    "    for weight, labels in zip(weights, labels_list):\n",
    "        for i in range(n_samples):\n",
    "            weighted_labels[i, labels[i]] += weight\n",
    "    \n",
    "    final_labels = np.argmax(weighted_labels, axis=1)\n",
    "    return final_labels\n",
    "\n",
    "\n",
    "# Ensemble Method 3: Advanced Affinity-based Spectral Clustering (AASC)\n",
    "def aasc_ensemble(labels_list, data, n_clusters):\n",
    "    \"\"\"\n",
    "    AASC: Constructs co-association matrix and applies spectral clustering.\n",
    "    Captures agreement across ensemble members through affinity representation.\n",
    "    \n",
    "    Process:\n",
    "    1. Build co-association matrix: A_ij = (1/M) * Σ I[C_m(i) = C_m(j)]\n",
    "    2. Apply spectral clustering to the affinity matrix\n",
    "    \"\"\"\n",
    "    n_samples = data.shape[0]\n",
    "    aggregated_affinity = np.zeros((n_samples, n_samples))\n",
    "    \n",
    "    # Build consensus affinity matrix\n",
    "    for labels in labels_list:\n",
    "        affinity_matrix = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                affinity_matrix[i, j] = 1 if labels[i] == labels[j] else 0\n",
    "        aggregated_affinity += affinity_matrix\n",
    "    \n",
    "    aggregated_affinity /= len(labels_list)\n",
    "    aggregated_affinity += 1e-5  # Numerical stability\n",
    "    \n",
    "    # Apply spectral clustering\n",
    "    spectral = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', random_state=42)\n",
    "    final_labels = spectral.fit_predict(aggregated_affinity)\n",
    "    \n",
    "    return final_labels\n",
    "\n",
    "\n",
    "# Ensemble Method 4: BIRCH-Optimized Hierarchical Consensus (BOHC/CSPA)\n",
    "def bohc_ensemble(labels_list, n_clusters):\n",
    "    \"\"\"\n",
    "    BOHC (also known as CSPA - Cluster-based Similarity Partitioning Algorithm).\n",
    "    Specifically designed for hierarchical clustering results from BIRCH.\n",
    "    \n",
    "    Preserves hierarchical structure information by building symmetric\n",
    "    co-association matrix and applying spectral clustering.\n",
    "    \"\"\"\n",
    "    n_samples = len(labels_list[0])\n",
    "    co_assoc_matrix = np.zeros((n_samples, n_samples))\n",
    "    \n",
    "    # Build co-association matrix (symmetric)\n",
    "    for labels in labels_list:\n",
    "        for i in range(n_samples):\n",
    "            for j in range(i + 1, n_samples):\n",
    "                if labels[i] == labels[j]:\n",
    "                    co_assoc_matrix[i, j] += 1\n",
    "                    co_assoc_matrix[j, i] += 1\n",
    "    \n",
    "    co_assoc_matrix /= len(labels_list)\n",
    "    co_assoc_matrix += 1e-5  # Numerical stability\n",
    "    \n",
    "    # Apply spectral clustering\n",
    "    spectral = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', random_state=42)\n",
    "    final_labels = spectral.fit_predict(co_assoc_matrix)\n",
    "    \n",
    "    return final_labels\n",
    "\n",
    "\n",
    "def calculate_ensemble_weights(data, labels_list, beta=5.0):\n",
    "    \"\"\"\n",
    "    Calculate weights for ensemble members based on silhouette scores.\n",
    "    Used for weighted voting ensemble.\n",
    "    \n",
    "    Weight formula: w_m = exp(β * S_m) / Σ exp(β * S_j)\n",
    "    where S_m is the silhouette score and β is temperature parameter (default: 5.0)\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    \n",
    "    for labels in labels_list:\n",
    "        try:\n",
    "            sil_score = silhouette_score(data, labels)\n",
    "            weights.append(np.exp(beta * sil_score))\n",
    "        except:\n",
    "            weights.append(0.0)\n",
    "    \n",
    "    # Normalize weights\n",
    "    total = sum(weights)\n",
    "    if total > 0:\n",
    "        weights = [w / total for w in weights]\n",
    "    else:\n",
    "        weights = [1.0 / len(weights)] * len(weights)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "\n",
    "def run_ensemble_consensus(data, birch_labels_dict, n_clusters_range=[5, 10, 15, 20]):\n",
    "    \"\"\"\n",
    "    Run all ensemble consensus strategies on BIRCH results.\n",
    "    \n",
    "    Applies four consensus methods:\n",
    "    1. Majority Voting (MV) - simple democratic voting\n",
    "    2. Weighted Voting (WV) - quality-weighted voting\n",
    "    3. AASC - affinity-based spectral clustering\n",
    "    4. BOHC/CSPA - hierarchical consensus optimized for BIRCH\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : ndarray\n",
    "        Input data for evaluation\n",
    "    birch_labels_dict : dict\n",
    "        Dictionary of BIRCH clustering labels\n",
    "    n_clusters_range : list\n",
    "        List of cluster counts to evaluate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ensemble_df : DataFrame\n",
    "        Results for all ensemble methods\n",
    "    ensemble_labels_dict : dict\n",
    "        Dictionary of ensemble labels\n",
    "    \"\"\"\n",
    "    ensemble_results = []\n",
    "    ensemble_labels_dict = {}\n",
    "    \n",
    "    # Group BIRCH labels by cluster count\n",
    "    labels_by_n_clusters = {n: [] for n in n_clusters_range}\n",
    "    for key, labels in birch_labels_dict.items():\n",
    "        n = int(key.split('_')[-1])\n",
    "        if n in n_clusters_range:\n",
    "            labels_by_n_clusters[n].append(labels)\n",
    "    \n",
    "    # Run ensemble methods for each cluster count\n",
    "    for n_clusters in n_clusters_range:\n",
    "        birch_labels = labels_by_n_clusters[n_clusters]\n",
    "        \n",
    "        if len(birch_labels) < 2:\n",
    "            print(f\"⚠ Skipping n={n_clusters}: insufficient BIRCH models\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate weights for weighted voting\n",
    "        weights = calculate_ensemble_weights(data, birch_labels)\n",
    "        \n",
    "        # Apply ensemble methods\n",
    "        ensemble_methods = {\n",
    "            'Majority_Voting': majority_voting_ensemble(birch_labels),\n",
    "            'Weighted_Voting': weighted_voting_ensemble(birch_labels, weights),\n",
    "            'AASC': aasc_ensemble(birch_labels, data, n_clusters),\n",
    "            'BOHC': bohc_ensemble(birch_labels, n_clusters)\n",
    "        }\n",
    "        \n",
    "        # Evaluate each ensemble method\n",
    "        for method_name, labels in ensemble_methods.items():\n",
    "            try:\n",
    "                model_key = f\"{method_name}_{n_clusters}\"\n",
    "                ensemble_labels_dict[model_key] = labels\n",
    "                \n",
    "                ensemble_results.append({\n",
    "                    'Model': method_name,\n",
    "                    'Clusters': n_clusters,\n",
    "                    'Silhouette': silhouette_score(data, labels),\n",
    "                    'Calinski-Harabasz': calinski_harabasz_score(data, labels),\n",
    "                    'Davies-Bouldin': davies_bouldin_score(data, labels)\n",
    "                })\n",
    "                \n",
    "                print(f\"✓ {model_key}: Silhouette={ensemble_results[-1]['Silhouette']:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Failed {method_name} with {n_clusters} clusters: {e}\")\n",
    "    \n",
    "    ensemble_df = pd.DataFrame(ensemble_results)\n",
    "    return ensemble_df, ensemble_labels_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dynamic Selection and Evaluation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model(results_df, criteria='Silhouette'):\n",
    "    \"\"\"\n",
    "    Select the best model based on clustering quality metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : DataFrame\n",
    "        Results with clustering metrics\n",
    "    criteria : str\n",
    "        Metric to optimize:\n",
    "        - 'Silhouette': Higher is better (range: -1 to 1)\n",
    "        - 'Calinski-Harabasz': Higher is better\n",
    "        - 'Davies-Bouldin': Lower is better\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_model : dict\n",
    "        Best model configuration and metrics\n",
    "    \"\"\"\n",
    "    if criteria == 'Davies-Bouldin':\n",
    "        # Lower is better for Davies-Bouldin\n",
    "        best_idx = results_df[criteria].idxmin()\n",
    "    else:\n",
    "        # Higher is better for Silhouette and Calinski-Harabasz\n",
    "        best_idx = results_df[criteria].idxmax()\n",
    "    \n",
    "    best_model = results_df.loc[best_idx].to_dict()\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def calculate_improvement(best_ensemble, best_birch):\n",
    "    \"\"\"\n",
    "    Calculate improvement percentage from BIRCH to ensemble.\n",
    "    \"\"\"\n",
    "    if best_birch['Silhouette'] == 0:\n",
    "        return 0.0\n",
    "    improvement = ((best_ensemble['Silhouette'] - best_birch['Silhouette']) / \n",
    "                   abs(best_birch['Silhouette']) * 100)\n",
    "    return improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete BIRCH-AE Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRCHAE:\n",
    "    \"\"\"\n",
    "    BIRCH-AE: Complete framework for hierarchical ensemble user segmentation.\n",
    "    \n",
    "    This class implements the full BIRCH-AE pipeline:\n",
    "    1. Data preprocessing\n",
    "    2. Autoencoder feature learning\n",
    "    3. BIRCH ensemble clustering\n",
    "    4. Consensus strategies (MV, WV, AASC, BOHC)\n",
    "    5. Dynamic model selection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=14, n_clusters_range=[5, 10, 15, 20]):\n",
    "        \"\"\"\n",
    "        Initialize BIRCH-AE framework.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        latent_dim : int\n",
    "            Dimensionality of autoencoder latent space (default: 14)\n",
    "        n_clusters_range : list\n",
    "            Range of cluster counts to evaluate (default: [5, 10, 15, 20])\n",
    "        \"\"\"\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_clusters_range = n_clusters_range\n",
    "        self.autoencoder = None\n",
    "        self.preprocessor = None\n",
    "        self.reducer = None\n",
    "        self.birch_results = None\n",
    "        self.ensemble_results = None\n",
    "        self.all_labels = {}\n",
    "    \n",
    "    def fit(self, filepath, user_id_col='user_id', reduction_method='autoencoder', \n",
    "            sample_size=None, use_autoencoder=True):\n",
    "        \"\"\"\n",
    "        Fit the complete BIRCH-AE pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to input CSV file\n",
    "        user_id_col : str\n",
    "            Name of user ID column\n",
    "        reduction_method : str\n",
    "            'autoencoder' or 'pca'\n",
    "        sample_size : int, optional\n",
    "            Sample size for large datasets\n",
    "        use_autoencoder : bool\n",
    "            Whether to use autoencoder for feature learning\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"BIRCH-AE Framework - Starting Pipeline\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Load and preprocess data\n",
    "        print(\"\\n[1/5] Loading and preprocessing data...\")\n",
    "        train_data, test_data, train_ids, test_ids, self.preprocessor = \\\n",
    "            load_and_preprocess_data(filepath, user_id_col, sample_size)\n",
    "        \n",
    "        # Combine train and test\n",
    "        full_data = np.vstack([train_data, test_data])\n",
    "        self.user_ids = np.concatenate([train_ids, test_ids])\n",
    "        \n",
    "        # Step 2: Dimensionality reduction\n",
    "        print(f\"\\n[2/5] Applying dimensionality reduction: {reduction_method}...\")\n",
    "        \n",
    "        if use_autoencoder and reduction_method == 'autoencoder':\n",
    "            # Build and train autoencoder\n",
    "            print(\"Building autoencoder architecture...\")\n",
    "            self.autoencoder = build_autoencoder(train_data.shape[1], self.latent_dim)\n",
    "            print(f\"Training autoencoder on {train_data.shape[0]} samples...\")\n",
    "            self.autoencoder, _ = train_autoencoder(self.autoencoder, train_data)\n",
    "            \n",
    "            # Encode data\n",
    "            print(\"Encoding data to latent space...\")\n",
    "            reduced_data = encode_data(self.autoencoder, full_data)\n",
    "            \n",
    "        elif reduction_method == 'pca':\n",
    "            train_reduced, self.reducer = reduce_dimensions_pca(train_data, self.latent_dim)\n",
    "            test_reduced = self.reducer.transform(test_data)\n",
    "            reduced_data = np.vstack([train_reduced, test_reduced])\n",
    "        \n",
    "        else:\n",
    "            reduced_data = full_data\n",
    "        \n",
    "        print(f\"Final data shape: {reduced_data.shape}\")\n",
    "        \n",
    "        # Step 3: BIRCH ensemble clustering\n",
    "        print(f\"\\n[3/5] Running BIRCH ensemble with threshold variations...\")\n",
    "        birch_results_df, birch_labels = run_birch_ensemble(reduced_data, self.n_clusters_range)\n",
    "        self.birch_results = birch_results_df\n",
    "        self.all_labels.update(birch_labels)\n",
    "        \n",
    "        # Step 4: Ensemble consensus strategies\n",
    "        print(f\"\\n[4/5] Running ensemble consensus strategies (MV, WV, AASC, BOHC)...\")\n",
    "        ensemble_results_df, ensemble_labels = run_ensemble_consensus(\n",
    "            reduced_data, birch_labels, self.n_clusters_range\n",
    "        )\n",
    "        self.ensemble_results = ensemble_results_df\n",
    "        self.all_labels.update(ensemble_labels)\n",
    "        \n",
    "        # Step 5: Evaluation and selection\n",
    "        print(f\"\\n[5/5] Evaluating results and selecting best model...\")\n",
    "        self._display_results()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _display_results(self):\n",
    "        \"\"\"Display comprehensive results summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"BIRCH Ensemble Results:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(self.birch_results.to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Ensemble Consensus Results:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(self.ensemble_results.to_string(index=False))\n",
    "        \n",
    "        # Best models\n",
    "        best_birch = select_best_model(self.birch_results)\n",
    "        best_ensemble = select_best_model(self.ensemble_results)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Best Models:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Best BIRCH Configuration: {best_birch['Model']} ({best_birch['Clusters']} clusters)\")\n",
    "        print(f\"  Threshold: {best_birch.get('Threshold', 'N/A')}\")\n",
    "        print(f\"  Silhouette: {best_birch['Silhouette']:.4f}\")\n",
    "        print(f\"  Calinski-Harabasz: {best_birch['Calinski-Harabasz']:.2f}\")\n",
    "        print(f\"  Davies-Bouldin: {best_birch['Davies-Bouldin']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nBest Ensemble Method: {best_ensemble['Model']} ({best_ensemble['Clusters']} clusters)\")\n",
    "        print(f\"  Silhouette: {best_ensemble['Silhouette']:.4f}\")\n",
    "        print(f\"  Calinski-Harabasz: {best_ensemble['Calinski-Harabasz']:.2f}\")\n",
    "        print(f\"  Davies-Bouldin: {best_ensemble['Davies-Bouldin']:.4f}\")\n",
    "        \n",
    "        improvement = calculate_improvement(best_ensemble, best_birch)\n",
    "        print(f\"\\n📊 Ensemble Improvement: {improvement:.2f}%\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    def get_labels(self, model_key):\n",
    "        \"\"\"\n",
    "        Get cluster labels for a specific model.\n",
    "        \n",
    "        Example: get_labels('BOHC_10') returns BOHC consensus with 10 clusters\n",
    "        \"\"\"\n",
    "        return self.all_labels.get(model_key)\n",
    "    \n",
    "    def get_user_segments(self, model_key):\n",
    "        \"\"\"\n",
    "        Get user-segment mapping for a specific model.\n",
    "        Returns DataFrame with user_id and segment columns.\n",
    "        \"\"\"\n",
    "        labels = self.get_labels(model_key)\n",
    "        if labels is None:\n",
    "            print(f\"Model '{model_key}' not found. Available models:\")\n",
    "            print(list(self.all_labels.keys()))\n",
    "            return None\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'user_id': self.user_ids,\n",
    "            'segment': labels\n",
    "        })\n",
    "    \n",
    "    def save_results(self, output_path):\n",
    "        \"\"\"\n",
    "        Save all results to CSV files.\n",
    "        Creates two files:\n",
    "        - {output_path}_metrics.csv: All clustering metrics\n",
    "        - {output_path}_segments.csv: User segments from best model\n",
    "        \"\"\"\n",
    "        # Save clustering metrics\n",
    "        all_results = pd.concat(\n",
    "            [self.birch_results.assign(Type='BIRCH'), \n",
    "             self.ensemble_results.assign(Type='Ensemble')],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        all_results.to_csv(f\"{output_path}_metrics.csv\", index=False)\n",
    "        \n",
    "        # Save best model labels\n",
    "        best_model = select_best_model(self.ensemble_results)\n",
    "        best_key = f\"{best_model['Model']}_{int(best_model['Clusters'])}\"\n",
    "        segments = self.get_user_segments(best_key)\n",
    "        \n",
    "        if segments is not None:\n",
    "            segments.to_csv(f\"{output_path}_segments.csv\", index=False)\n",
    "            print(f\"\\n✓ Results saved:\")\n",
    "            print(f\"  - {output_path}_metrics.csv\")\n",
    "            print(f\"  - {output_path}_segments.csv\")\n",
    "        else:\n",
    "            print(f\"\\n✗ Failed to save segments: model {best_key} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example Usage\n",
    "\n",
    "Below is a complete example showing how to use the BIRCH-AE framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Running the complete BIRCH-AE pipeline\n",
    "# \n",
    "# # Initialize the BIRCH-AE framework\n",
    "# birch_ae = BIRCHAE(\n",
    "#     latent_dim=14,                    # Dimensionality of latent space\n",
    "#     n_clusters_range=[5, 10, 15, 20]  # Range of cluster counts to evaluate\n",
    "# )\n",
    "# \n",
    "# # Fit the framework on e-commerce user data\n",
    "# birch_ae.fit(\n",
    "#     filepath='ecommerce_user_metrics.csv',  # Path to your CSV file\n",
    "#     user_id_col='visitorid',                # Name of user ID column\n",
    "#     reduction_method='autoencoder',         # Use 'autoencoder' or 'pca'\n",
    "#     sample_size=50000,                      # Optional: sample size for large datasets\n",
    "#     use_autoencoder=True                    # Enable autoencoder feature learning\n",
    "# )\n",
    "# \n",
    "# # Get user segments for best ensemble model (automatically selected)\n",
    "# # Or specify a particular model, e.g., 'BOHC_10' for BOHC with 10 clusters\n",
    "# segments_df = birch_ae.get_user_segments('BOHC_10')\n",
    "# print(\"\\nSegment Distribution:\")\n",
    "# print(segments_df['segment'].value_counts())\n",
    "# \n",
    "# # Save all results to files\n",
    "# birch_ae.save_results('output/birch_ae_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Advanced Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 1: Using PCA instead of Autoencoder (faster, linear)\n",
    "# birch_ae_pca = BIRCHAE(latent_dim=14)\n",
    "# birch_ae_pca.fit(\n",
    "#     filepath='retail_rocket_user_metrics.csv',\n",
    "#     user_id_col='visitorid',\n",
    "#     reduction_method='pca',\n",
    "#     use_autoencoder=False  # Skip autoencoder training\n",
    "# )\n",
    "# \n",
    "# # Example 2: Large-Scale Dataset (millions of users)\n",
    "# birch_ae_large = BIRCHAE(n_clusters_range=[5, 10, 15, 20, 25])\n",
    "# birch_ae_large.fit(\n",
    "#     filepath='large_ecommerce_dataset.csv',\n",
    "#     sample_size=100000,  # Sample for manageable computation\n",
    "#     reduction_method='autoencoder'\n",
    "# )\n",
    "# \n",
    "# # Example 3: Comparing Multiple Ensemble Methods\n",
    "# ensemble_methods = ['Majority_Voting_10', 'Weighted_Voting_10', 'AASC_10', 'BOHC_10']\n",
    "# \n",
    "# print(\"\\nComparing Ensemble Methods (10 clusters):\")\n",
    "# for method in ensemble_methods:\n",
    "#     segments = birch_ae.get_user_segments(method)\n",
    "#     if segments is not None:\n",
    "#         print(f\"\\n{method}:\")\n",
    "#         print(segments['segment'].value_counts().sort_index())\n",
    "# \n",
    "# # Example 4: Analyzing BIRCH Threshold Impact\n",
    "# print(\"\\nBIRCH Threshold Analysis:\")\n",
    "# threshold_configs = ['BIRCH_Fine-Grained_10', 'BIRCH_Balanced_10', 'BIRCH_Coarse-Grained_10']\n",
    "# \n",
    "# for config in threshold_configs:\n",
    "#     labels = birch_ae.get_labels(config)\n",
    "#     if labels is not None:\n",
    "#         print(f\"\\n{config}:\")\n",
    "#         print(f\"  Unique segments: {len(np.unique(labels))}\")\n",
    "#         print(f\"  Distribution: {np.bincount(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Framework Notes and Best Practices\n",
    "\n",
    "### BIRCH-AE Architecture:\n",
    "The framework follows a hierarchical approach:\n",
    "1. **Preprocessing**: StandardScaler for numeric, OneHotEncoder for categorical\n",
    "2. **Feature Learning**: Deep autoencoder (512→256→128→latent→128→256→output)\n",
    "3. **BIRCH Ensemble**: Multiple thresholds (0.3, 0.5, 0.8) capture different granularities\n",
    "4. **Consensus**: Four strategies (MV, WV, AASC, BOHC) aggregate base clusterings\n",
    "5. **Dynamic Selection**: Automatic best model selection via multi-criteria evaluation\n",
    "\n",
    "### Parameter Recommendations:\n",
    "- **latent_dim**: 10-20 works well (compression ratio 2:1 to 4:1)\n",
    "- **n_clusters_range**: Test 5-25 clusters for e-commerce\n",
    "- **BIRCH thresholds**:\n",
    "  - 0.3 (fine-grained): Many small, homogeneous clusters\n",
    "  - 0.5 (balanced): Moderate granularity (recommended starting point)\n",
    "  - 0.8 (coarse-grained): Fewer, broader clusters\n",
    "- **branching_factor**: 50 (default) balances tree depth and memory\n",
    "\n",
    "### Evaluation Metrics:\n",
    "- **Silhouette Score** (range: -1 to 1, higher is better):\n",
    "  - > 0.5: Strong, well-separated clusters\n",
    "  - 0.25-0.5: Reasonable structure\n",
    "  - < 0.25: Weak or overlapping clusters\n",
    "- **Calinski-Harabasz Index**: Higher is better (no fixed range)\n",
    "- **Davies-Bouldin Index**: Lower is better (0 to ∞)\n",
    "\n",
    "### Computational Considerations:\n",
    "- **Memory Efficiency**: BIRCH's CF Tree requires O(n) space\n",
    "- **Time Complexity**: \n",
    "  - BIRCH: O(n log n) for tree construction\n",
    "  - Autoencoder: O(epochs × batch_size × features)\n",
    "  - Consensus: O(n² × M) for affinity-based methods (AASC, BOHC)\n",
    "- **Scalability**: For >100k users, consider sampling or use PCA instead of autoencoder\n",
    "- **Incremental Learning**: BIRCH supports online updates for streaming data\n",
    "\n",
    "### When to Use BIRCH-AE:\n",
    "✓ E-commerce user segmentation with high-dimensional behavioral features\n",
    "✓ Large-scale datasets (millions of users)\n",
    "✓ Correlated features requiring non-linear dimensionality reduction\n",
    "✓ Need for hierarchical multi-granularity segmentation\n",
    "✓ Streaming data with incremental updates\n",
    "\n",
    "### Citation:\n",
    "If you use this framework in your research, please cite:\n",
    "```\n",
    "Li, C. et al. (2025). BIRCH-AE: A Scalable Hierarchical Ensemble Framework\n",
    "for E-Commerce User Segmentation with Autoencoder Feature Learning.\n",
    "IEEE Access. [Under Review]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
